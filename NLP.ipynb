{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNR9IUrWawFp+3JymvYpyNC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AJISJPRAKASH/Esportas/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deRwm6xe4Mz2"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Eh2E1UDm4RSw",
        "outputId": "f5ac300c-1575-4a2d-87c7-193019586673"
      },
      "source": [
        "sentence='Hai AJi ,How are you ,Are you  studying'\n",
        "lsentence=sentence.lower()\n",
        "lsentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hai aji ,how are you ,are you  studying'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekHguxtF4Y-J",
        "outputId": "36bf2c61-67e8-4d7a-85c1-f0906e6342a2"
      },
      "source": [
        "tokens =[]\n",
        "for t in lsentence.split():\n",
        "\ttokens.append(t)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hai', 'aji', ',how', 'are', 'you', ',are', 'you', 'studying']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56w7nVWh4hQq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c666de47-01e4-41f3-cc0a-ea5bb7c4906f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words =stopwords.words('english')\n",
        "clean_tokens=tokens[:]\n",
        "for token in tokens:\n",
        "\tif token in stop_words:\n",
        "\t\tclean_tokens.remove(token)\n",
        "print('\\n')\n",
        "print(clean_tokens)\n",
        "print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "\n",
            "['hai', 'aji', ',how', ',are', 'studying']\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX8OFL5W7m7M",
        "outputId": "47d59667-d1b4-4f45-ef6b-a072a17dab57"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "tagged=nltk.pos_tag(clean_tokens)\n",
        "print('\\n')\n",
        "print(tagged)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "\n",
            "\n",
            "[('hai', 'NN'), ('aji', 'VBZ'), (',how', 'NNP'), (',are', 'NNP'), ('studying', 'VBG')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PqwODZd8zqg",
        "outputId": "6b92aff4-a256-43c1-e819-baeaff89ba10"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "filtered_sentence =[]\n",
        "for w in tokens:\n",
        "\tif w not in stop_words:\n",
        "\t\tfiltered_sentence.append(w)\n",
        "Stem_words = []\n",
        "ps =PorterStemmer()\n",
        "for w in filtered_sentence:\n",
        "\trootWord=ps.stem(w)\n",
        "\tStem_words.append(rootWord)\n",
        "print(filtered_sentence)\n",
        "print('\\n')\n",
        "print(Stem_words)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hai', 'aji', ',how', ',are', 'studying']\n",
            "\n",
            "\n",
            "['hai', 'aji', ',how', ',are', 'studi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW6BqAXwD1mc",
        "outputId": "643397f7-aa7e-44c9-adab-f47f4c3be304"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "filtered_sentence = []\n",
        "for w in tokens:\n",
        "\tif w not in stop_words:\n",
        "\t\tfiltered_sentence.append(w)\n",
        "print(filtered_sentence)\n",
        "\n",
        "lemma_word = []\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "for w in filtered_sentence:\n",
        "\tword1=wordnet_lemmatizer.lemmatize(w,pos= \"n\")\n",
        "\tword2=wordnet_lemmatizer.lemmatize(word1,pos= \"v\")\n",
        "\tword3=wordnet_lemmatizer.lemmatize(word2,pos= (\"a\"))\n",
        "\tlemma_word.append(word3)\n",
        "print('\\n')\n",
        "print(lemma_word)\n",
        "len(lemma_word)\t\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hai', 'aji', ',how', ',are', 'studying']\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "\n",
            "\n",
            "['hai', 'aji', ',how', ',are', 'study']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tPz1br2GRmR"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import pandas as pd\n",
        " \n",
        "vocab =set(lemma_word)\n",
        "print(vocab)\n",
        "print('\\n')\n",
        "vocab =pd.Series(range(len(vocab)), index=vocab)\n",
        "word_ids =vocab.loc[lemma_word].values\n",
        "\n",
        "print(word_ids)\n",
        "\n",
        "inputs = tf.placeholder(tf.int32, [None])\n",
        "one_hot_inputs =tf.one_hot(inputs, len(vocab))\n",
        "transformed =tf.Session().run(one_hot_inputs,{inputs: words_ids})\n",
        "print('\\n')\n",
        "print(transformed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19m2dW5_J5VQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}